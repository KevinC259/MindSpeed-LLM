{
  "os": "Linux-5.10.0-182.0.0.95.r1941_123.hce2.aarch64-aarch64-with-glibc2.35",
  "python": "CPython 3.10.19",
  "startedAt": "2025-12-24T16:50:29.550001Z",
  "args": [
    "--use-mcore-models",
    "--sequence-parallel",
    "--spec",
    "mindspeed_llm.tasks.models.spec.qwen3_spec",
    "layer_spec",
    "--kv-channels",
    "128",
    "--qk-layernorm",
    "--num-layers",
    "28",
    "--hidden-size",
    "1024",
    "--num-attention-heads",
    "16",
    "--ffn-hidden-size",
    "3072",
    "--max-position-embeddings",
    "32768",
    "--make-vocab-size-divisible-by",
    "1",
    "--padded-vocab-size",
    "151936",
    "--rotary-base",
    "1000000",
    "--disable-bias-linear",
    "--swiglu",
    "--tokenizer-type",
    "PretrainedFromHF",
    "--tokenizer-name-or-path",
    "/sharedata/data/models/Qwen3-0.6B-Base",
    "--normalization",
    "RMSNorm",
    "--position-embedding-type",
    "rope",
    "--norm-epsilon",
    "1e-6",
    "--no-gradient-accumulation-fusion",
    "--attention-softmax-in-fp32",
    "--exit-on-missing-checkpoint",
    "--group-query-attention",
    "--num-query-groups",
    "8",
    "--no-load-optim",
    "--no-load-rng",
    "--seed",
    "42",
    "--optimizer-selection",
    "zetas",
    "--bf16",
    "--data-path",
    "/sharedata/ckw/dataset/openwebtext_text_document",
    "--split",
    "100,0,0",
    "--log-interval",
    "1",
    "--save-interval",
    "400",
    "--eval-interval",
    "2000",
    "--eval-iters",
    "0",
    "--tensorboard-dir",
    "./tensorboard/qwen3_06b_4k_mcore_ptd",
    "--log-timers-to-tensorboard",
    "--log-throughput",
    "--use-wandb",
    "--wandb-project",
    "qwen3",
    "--wandb-exp-name",
    "zetas_qwen3_0.6b_4k_3e-4_openwebtext",
    "--wandb-entity",
    "584272225-south-china-university-of-technology",
    "--wandb-group",
    "private",
    "--wandb-save-dir",
    "./wandb/zetas_qwen3_0.6b_4k_3e-4_openwebtext",
    "--use-flash-attn",
    "--use-fused-rotary-pos-emb",
    "--use-rotary-position-embeddings",
    "--use-fused-swiglu",
    "--use-fused-rmsnorm",
    "--no-masked-softmax-fusion",
    "--use-distributed-optimizer",
    "--micro-batch-size",
    "1",
    "--global-batch-size",
    "64",
    "--lr",
    "3e-4",
    "--min-lr",
    "1.25e-7",
    "--weight-decay",
    "1e-1",
    "--attention-dropout",
    "0.0",
    "--hidden-dropout",
    "0.0",
    "--clip-grad",
    "1.0",
    "--adam-beta1",
    "0.9",
    "--adam-beta2",
    "0.95",
    "--initial-loss-scale",
    "4096",
    "--lr-warmup-fraction",
    "0.01",
    "--seed",
    "42",
    "--bf16",
    "--train-iters",
    "20000",
    "--seq-length",
    "4096",
    "--no-shared-storage",
    "--tensor-model-parallel-size",
    "1",
    "--pipeline-model-parallel-size",
    "4",
    "--lr-warmup-fraction",
    "0.003",
    "--lr-warmup-init",
    "6e-6",
    "--distributed-backend",
    "nccl",
    "--log-throughput",
    "--save",
    "/sharedata/ckw/ckpts/qwen3_0.6B_zetas_lr3e-4_openwebtext"
  ],
  "program": "/sharedata/ckw/MindSpeed-LLM/pretrain_gpt.py",
  "codePath": "pretrain_gpt.py",
  "git": {
    "remote": "https://gitcode.com/ascend/MindSpeed-LLM.git",
    "commit": "61fd362e4993f124c548cfb1628a3df78b7a919c"
  },
  "email": "584272225@qq.com",
  "root": "./wandb/zetas_qwen3_0.6b_4k_3e-4_openwebtext",
  "host": "qiuwu-optimizer-dev-server-28-server-0",
  "executable": "/sharedata/shareenvs/qiuwu-optimizer-dev/bin/python3.10",
  "cpu_count": 320,
  "cpu_count_logical": 320,
  "disk": {
    "/": {
      "total": "1055731638272",
      "used": "239985053696"
    }
  },
  "memory": {
    "total": "2162447699968"
  },
  "writerId": "qm0a1devkuwf64obx1nhu0kby82xgam9"
}